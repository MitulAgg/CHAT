import os
from dotenv import load_dotenv
from google.adk.agents import LlmAgent
from google.adk.tools import google_search
from google.adk.tools.langchain_tool import LangchainTool
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA


load_dotenv()

# # Step 1: Load and process the text file
def load_and_split_text(file_path):
    loader = TextLoader(file_path,encoding="utf-8")  # Ensure the file is read with the correct encoding
    documents = loader.load()
    # print(documents)
      # Debug: Check loaded documents
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # Adjust based on your needs
        chunk_overlap=200  # Overlap to maintain context
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

# Step 2: Create a vector store for RAG
def create_vector_store(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = Chroma.from_documents(chunks, embeddings)
    return vector_store

# Step 3: Set up the RAG pipeline with LangChain
def setup_rag_pipeline(vector_store):

    prompt_template = """
    You are a helpful assistant. Use the following context to answer the user's question accurately.
    If the answer is not in the context, say so and avoid making up information.

    Context: {context}

    Question: {question}

    Answer:
    """
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    
    rag_chain = RetrievalQA.from_chain_type(
        llm=ChatGoogleGenerativeAI(model="gemini-1.5-flash"),  # LLM will be provided by ADK agent
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),  # Retrieve top 3 relevant chunks
        chain_type_kwargs={"prompt": prompt}
    )
    
    def rag_query_function(query: str) -> str:
        """
        A simple function to query the RAG pipeline.
        
        Args:
            query (str): The user's question.
        
        Returns:
            str: The answer generated by the RAG pipeline.
        """
        result = rag_chain.invoke(query)
        return result
    return rag_query_function




# # question_answering_agent
# with open(r"scraped_data.txt", "r", encoding="utf-8") as f:
#         content = f.read().strip()
#         if content == "temp":
#             print("No content found in the text file. Using Google Search tool instead.")
#             question_answering_agent = LlmAgent(
#                 name="question_answering_agent",
#                 model="gemini-2.0-flash",
#                 description="Question answering agent using google search and user preferences",
#                 instruction="""
#                 You are a helpful assistant that answers questions about the user's preferences .
              
#                 Here is some information about the user's preferences:
#                 Preferences: 
#                 {user_preferences}
#                 """,
#                 tools=[google_search]
#             )
#         else:
#             print("Content found in the text file. Using RAG tool for question answering.")
#             question_answering_agent = LlmAgent(
#                 name="question_answering_agent",
#                 model="gemini-2.0-flash",
#                 description="Question answering agent using RAG from a text file",
#                 instruction="""
#                 You are a helpful assistant that answers questions about the user's preferences and content from a text file.
#                 Use the RAG tool to retrieve relevant context and provide accurate answers.
#                 If the answer is not in the text or preferences, say so clearly.

#                 Here is some information about the user's preferences:
#                 Preferences: 
#                 {user_preferences}
#                 """,
#                 tools=[rag_tool]
#             )


